{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "nuclear-chapter",
   "metadata": {
    "id": "nuclear-chapter"
   },
   "source": [
    "## Relation Extraction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6-f3BeyZrg3D",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-f3BeyZrg3D",
    "outputId": "89166912-50f1-4e12-91ef-f5ab3546c680"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  1 04:51:34 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   43C    P0    47W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LoB4Wr8mNjfH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "id": "LoB4Wr8mNjfH",
    "outputId": "d74a2410-9462-4bda-e58d-86de8308bfa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 1.10.0\n",
      "Uninstalling torch-1.10.0:\n",
      "  Would remove:\n",
      "    /usr/local/bin/convert-caffe2-to-onnx\n",
      "    /usr/local/bin/convert-onnx-to-caffe2\n",
      "    /usr/local/bin/torchrun\n",
      "    /usr/local/lib/python3.7/dist-packages/caffe2/*\n",
      "    /usr/local/lib/python3.7/dist-packages/torch-1.10.0.dist-info/*\n",
      "    /usr/local/lib/python3.7/dist-packages/torch/*\n",
      "Proceed (y/n)? y\n",
      "  Successfully uninstalled torch-1.10.0\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.9.0+cu111\n",
      "tcmalloc: large alloc 2041348096 bytes == 0x555ad5a8a000 @  0x7fb0536201e7 0x555ad304ef98 0x555ad3019e27 0x555ad3198115 0x555ad3132888 0x555ad301d6f2 0x555ad30fbc6e 0x555ad301d349 0x555ad310ee1d 0x555ad3090e99 0x555ad301eafa 0x555ad308cc0d 0x555ad301eafa 0x555ad308cc0d 0x555ad308bced 0x555ad301f48c 0x555ad301f698 0x555ad308dfe4 0x555ad308b9ee 0x555ad301f48c 0x555ad301f698 0x555ad308dfe4 0x555ad308b9ee 0x555ad301f48c 0x555ad301f698 0x555ad308dfe4 0x555ad308bced 0x555ad301f48c 0x555ad301f698 0x555ad308dfe4 0x555ad308b9ee\n",
      "tcmalloc: large alloc 2041348096 bytes == 0x555b4f552000 @  0x7fb0536201e7 0x555ad304ef98 0x555ad3019e27 0x555ad3130227 0x555ad301d46c 0x555ad310ee1d 0x555ad3090e99 0x555ad301eafa 0x555ad308cc0d 0x555ad301eafa 0x555ad308cc0d 0x555ad308bced 0x555ad301f48c 0x555ad301f698 0x555ad308dfe4 0x555ad308b9ee 0x555ad301f48c 0x555ad301f698 0x555ad308dfe4 0x555ad308b9ee 0x555ad301f48c 0x555ad301f698 0x555ad308dfe4 0x555ad308bced 0x555ad301f48c 0x555ad301f698 0x555ad308dfe4 0x555ad308b9ee 0x555ad301ebda 0x555ad308d737 0x555ad301eafa\n",
      "tcmalloc: large alloc 2041348096 bytes == 0x555bc901a000 @  0x7fb0536201e7 0x555ad3050008 0x555ad310a82e 0x555ad301d2ed 0x555ad310ee1d 0x555ad3090e99 0x555ad301eafa 0x555ad308cc0d 0x555ad308b9ee 0x555ad301ebda 0x555ad308d737 0x555ad301eafa 0x555ad308c915 0x555ad301eafa 0x555ad308cc0d 0x555ad301eafa 0x555ad308cc0d 0x555ad308bced 0x555ad301f48c 0x555ad301f698 0x555ad308dfe4 0x555ad308b9ee 0x555ad301f48c 0x555ad301f698 0x555ad308dfe4 0x555ad308b9ee 0x555ad301f48c 0x555ad301f698 0x555ad308dfe4 0x555ad308bced 0x555ad301f48c\n",
      "tcmalloc: large alloc 2041348096 bytes == 0x555c42ae2000 @  0x7fb0536201e7 0x555ad304ef98 0x555ad310b21c 0x555ad3105409 0x555ad308ce7a 0x555ad308b9ee 0x555ad301ebda 0x555ad308cc0d 0x555ad308b9ee 0x555ad301ebda 0x555ad308cc0d 0x555ad308bced 0x555ad301ebda 0x555ad308cc0d 0x555ad308bced 0x555ad301ebda 0x555ad308cc0d 0x555ad308bced 0x555ad301ebda 0x555ad308cc0d 0x555ad308b9ee 0x555ad301ebda 0x555ad308d737 0x555ad301eafa 0x555ad308c915 0x555ad301eafa 0x555ad308cc0d 0x555ad301eafa 0x555ad308cc0d 0x555ad308bced 0x555ad301f48c\n",
      "tcmalloc: large alloc 2041348096 bytes == 0x555cbce74000 @  0x7fb0536201e7 0x555ad304ef98 0x555ad3019e27 0x555ad312ff2a 0x555ad301ca6f 0x555ad305d045 0x555ad301dc52 0x555ad3090c25 0x555ad308bced 0x555ad301ebda 0x555ad308cc0d 0x555ad308bced 0x555ad301ebda 0x555ad308cc0d 0x555ad308bced 0x555ad301ebda 0x555ad308cc0d 0x555ad308b9ee 0x555ad301ebda 0x555ad308d737 0x555ad301eafa 0x555ad308c915 0x555ad301eafa 0x555ad308cc0d 0x555ad301eafa 0x555ad308cc0d 0x555ad308bced 0x555ad301f48c 0x555ad301f698 0x555ad308dfe4 0x555ad308b9ee\n",
      "tcmalloc: large alloc 2041348096 bytes == 0x555cbce74000 @  0x7fb0536201e7 0x555ad304ef98 0x555ad3019e27 0x555ad3198115 0x555ad3132888 0x555ad301d6f2 0x555ad30fbc6e 0x555ad301d349 0x555ad310ee1d 0x555ad3090e99 0x555ad301eafa 0x555ad308cc0d 0x555ad301eafa 0x555ad308cc0d 0x555ad308bced 0x555ad301f48c 0x555ad301f698 0x555ad308dfe4 0x555ad308b9ee 0x555ad301f48c 0x555ad301f698 0x555ad308dfe4 0x555ad308b9ee 0x555ad301f48c 0x555ad301f698 0x555ad308dfe4 0x555ad308bced 0x555ad301f48c 0x555ad301f698 0x555ad308dfe4 0x555ad308b9ee\n",
      "tcmalloc: large alloc 2551685120 bytes == 0x555ad5a8a000 @  0x7fb053621615 0x555ad301a4cc 0x555ad30fa47a 0x555ad301d2ed 0x555ad310ee1d 0x555ad3090e99 0x555ad308b9ee 0x555ad301ebda 0x555ad308cc0d 0x555ad308b9ee 0x555ad301ebda 0x555ad308cc0d 0x555ad308b9ee 0x555ad301ebda 0x555ad308cc0d 0x555ad308b9ee 0x555ad301ebda 0x555ad308cc0d 0x555ad308b9ee 0x555ad301ebda 0x555ad308cc0d 0x555ad301eafa 0x555ad308cc0d 0x555ad308b9ee 0x555ad301ebda 0x555ad308d737 0x555ad308b9ee 0x555ad301ebda 0x555ad308cc0d 0x555ad308bced 0x555ad301ebda\n",
      "  Using cached https://download.pytorch.org/whl/cu111/torch-1.9.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2041.3 MB)\n",
      "Requirement already satisfied: torchvision==0.10.0+cu111 in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n",
      "Requirement already satisfied: torchaudio==0.9.0 in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0+cu111) (3.10.0.2)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.0+cu111) (7.1.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.0+cu111) (1.19.5)\n",
      "Installing collected packages: torch\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.9.0+cu111 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.9.0+cu111\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "torch"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip uninstall torch\n",
    "!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aGdBWoHcOwUx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aGdBWoHcOwUx",
    "outputId": "6d7954b9-b411-49fc-f89b-f0c8093922bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torchtext 0.11.0\n",
      "Uninstalling torchtext-0.11.0:\n",
      "  Would remove:\n",
      "    /usr/local/lib/python3.7/dist-packages/torchtext-0.11.0.dist-info/*\n",
      "    /usr/local/lib/python3.7/dist-packages/torchtext/*\n",
      "Proceed (y/n)? y\n",
      "  Successfully uninstalled torchtext-0.11.0\n",
      "Collecting torchtext==0.10.0\n",
      "  Downloading torchtext-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.6 MB 5.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.19.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (2.23.0)\n",
      "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.9.0+cu111)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext==0.10.0) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2.10)\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall torchtext\n",
    "!pip install torchtext==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yr-mMu47NnIv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yr-mMu47NnIv",
    "outputId": "12de88ef-84ba-4e66-9e35-8ecc83c0dc9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.10.3\n",
      "  Downloading transformers-4.10.3-py3-none-any.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 5.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.3) (4.62.3)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 71.4 MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 74.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.3) (2019.12.20)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[K     |████████████████████████████████| 596 kB 75.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.3) (21.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.3) (3.4.0)\n",
      "Collecting huggingface-hub>=0.0.12\n",
      "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 9.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.3) (1.19.5)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.3) (4.8.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.3) (2.23.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers==4.10.3) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.10.3) (3.0.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.10.3) (3.6.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.3) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.3) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.3) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.3) (2.10)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.3) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.3) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.3) (1.15.0)\n",
      "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.10.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SXrbPkBFNnBg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SXrbPkBFNnBg",
    "outputId": "6477bdf8-9b90-4604-f5e2-98808f9d0736"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_lightning==1.2.8\n",
      "  Downloading pytorch_lightning-1.2.8-py3-none-any.whl (841 kB)\n",
      "\u001b[K     |████████████████████████████████| 841 kB 5.1 MB/s \n",
      "\u001b[?25hCollecting future>=0.17.1\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 60.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: PyYAML!=5.4.*,>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.2.8) (6.0)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.2.8) (2.7.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.2.8) (1.19.5)\n",
      "Collecting torchmetrics>=0.2.0\n",
      "  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n",
      "\u001b[K     |████████████████████████████████| 329 kB 56.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.2.8) (1.10.0+cu111)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.2.8) (4.62.3)\n",
      "Collecting fsspec[http]>=0.8.1\n",
      "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 68.8 MB/s \n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 59.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning==1.2.8) (2.23.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.2.8) (0.37.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.2.8) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.2.8) (1.0.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.2.8) (3.17.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.2.8) (57.4.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.2.8) (3.3.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.2.8) (0.4.6)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.2.8) (1.42.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.2.8) (0.12.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.2.8) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.2.8) (1.35.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning==1.2.8) (1.15.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.2.8) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.2.8) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.2.8) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning==1.2.8) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning==1.2.8) (4.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning==1.2.8) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning==1.2.8) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.2.8) (0.4.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]>=0.8.1->pytorch_lightning==1.2.8) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]>=0.8.1->pytorch_lightning==1.2.8) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]>=0.8.1->pytorch_lightning==1.2.8) (2021.10.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]>=0.8.1->pytorch_lightning==1.2.8) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning==1.2.8) (3.1.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics>=0.2.0->pytorch_lightning==1.2.8) (21.3)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch_lightning==1.2.8) (2.0.8)\n",
      "Collecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
      "\u001b[K     |████████████████████████████████| 271 kB 77.7 MB/s \n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
      "\u001b[K     |████████████████████████████████| 160 kB 81.8 MB/s \n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch_lightning==1.2.8) (21.2.0)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
      "\u001b[K     |████████████████████████████████| 192 kB 84.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics>=0.2.0->pytorch_lightning==1.2.8) (3.0.6)\n",
      "Building wheels for collected packages: future\n",
      "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=ff93072d54ae662b9fbb4e7f6c20b639efc94037b784414c371b6744173b0378\n",
      "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "Successfully built future\n",
      "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, torchmetrics, future, pytorch-lightning\n",
      "  Attempting uninstall: future\n",
      "    Found existing installation: future 0.16.0\n",
      "    Uninstalling future-0.16.0:\n",
      "      Successfully uninstalled future-0.16.0\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 frozenlist-1.2.0 fsspec-2021.11.1 future-0.18.2 multidict-5.2.0 pytorch-lightning-1.2.8 torchmetrics-0.6.0 yarl-1.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_lightning==1.2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27DG9efTNslX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27DG9efTNslX",
    "outputId": "c39e1a7f-9e84-45e5-cebc-ce5eb93b9a86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-prompt",
   "metadata": {
    "id": "identical-prompt"
   },
   "outputs": [],
   "source": [
    "### torch == 1.9.1\n",
    "### transformers == 4.10.3\n",
    "### pytorch_lightning == 1.2.8\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import easydict\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics.functional import accuracy, f1, auroc\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TB4yuN1ySbX2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "TB4yuN1ySbX2",
    "outputId": "9b1028a1-838a-4f04-ec72-a1514dd8d445"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'1.10.0+cu111'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f_Rr5rVhSdbB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "f_Rr5rVhSdbB",
    "outputId": "8b1e2deb-2f60-41a6-9d90-4897396d2e19"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'1.2.8'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-wales",
   "metadata": {
    "id": "controversial-wales"
   },
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    'seed': 42\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-jungle",
   "metadata": {
    "id": "instrumental-jungle"
   },
   "outputs": [],
   "source": [
    "args.batch_size = 12\n",
    "args.hidden_size = 768\n",
    "args.n_class = 97\n",
    "args.num_workers = 2\n",
    "args.epochs = 5\n",
    "args.train = True\n",
    "args.bert_model = 'snunlp/KR-Medium'\n",
    "args.max_token_len = 512\n",
    "args.train_data = '/content/drive/MyDrive/Korean_RE/data/data_split/train.csv' \n",
    "args.val_data = '/content/drive/MyDrive/Korean_RE/data/data_split/val.csv'\n",
    "args.test_data = '/content/drive/MyDrive/Korean_RE/data/data_split/test.csv'\n",
    "args.relation_list = '/content/drive/MyDrive/Korean_RE/data/relation/relation_list.txt'\n",
    "args.save_dir = '/content/drive/MyDrive/Korean_RE/ckpt/whole_data'\n",
    "args.log_file = '/content/drive/MyDrive/Korean_RE/log/whole_data_result.txt'\n",
    "args.mode = \"ALLCC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-creation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "concerned-creation",
    "outputId": "a00ce32a-2ee9-46bc-f7d0-607afb3b84fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 12,\n",
       " 'bert_model': 'snunlp/KR-Medium',\n",
       " 'epochs': 5,\n",
       " 'hidden_size': 768,\n",
       " 'log_file': '/content/drive/MyDrive/Korean_RE/log/whole_data_result.txt',\n",
       " 'max_token_len': 512,\n",
       " 'mode': 'ALLCC',\n",
       " 'n_class': 97,\n",
       " 'num_workers': 2,\n",
       " 'relation_list': '/content/drive/MyDrive/Korean_RE/data/relation/relation_list.txt',\n",
       " 'save_dir': '/content/drive/MyDrive/Korean_RE/ckpt/whole_data',\n",
       " 'seed': 42,\n",
       " 'test_data': '/content/drive/MyDrive/Korean_RE/data/data_split/test.csv',\n",
       " 'train': True,\n",
       " 'train_data': '/content/drive/MyDrive/Korean_RE/data/data_split/train.csv',\n",
       " 'val_data': '/content/drive/MyDrive/Korean_RE/data/data_split/val.csv'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-explosion",
   "metadata": {
    "id": "artificial-explosion"
   },
   "source": [
    "## 각 문장을 BERT를 통과할 수 있는 형태로 변형"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-converter",
   "metadata": {
    "id": "practical-converter"
   },
   "source": [
    "함수로 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-genre",
   "metadata": {
    "id": "necessary-genre"
   },
   "outputs": [],
   "source": [
    "def entity_markers_added(sent: str, subj_range: list, obj_range: list) -> str:\n",
    "    \"\"\" 문장과 관계를 구하고자 하는 두 개체의 인덱스 범위가 주어졌을 때 entity marker token을 추가하여 반환하는 함수.\n",
    "    \n",
    "    Example:\n",
    "        sent = '모토로라 레이저 M는 모토로라 모빌리티에서 제조/판매하는 안드로이드 스마트폰이다.'\n",
    "        subj_range = [0, 10]   # sent[subj_range[0]: subj_range[1]] => '모토로라 레이저 M'\n",
    "        obj_range = [12, 21]   # sent[obj_range[0]: obj_range[1]] => '모토로라 모빌리티'\n",
    "        \n",
    "    Return:\n",
    "        '[E1] 모토로라 레이저 M [/E1] 는  [E2] 모토로라 모빌리티 [/E2] 에서 제조/판매하는 안드로이드 스마트폰이다.'\n",
    "    \"\"\"\n",
    "    result_sent = ''\n",
    "    \n",
    "    for i, char in enumerate(sent):\n",
    "        if i == subj_range[0]:\n",
    "            result_sent += ' [E1] '\n",
    "        elif i == subj_range[1]:\n",
    "            result_sent += ' [/E1] '\n",
    "        if i == obj_range[0]:\n",
    "            result_sent += ' [E2] '\n",
    "        elif i == obj_range[1]:\n",
    "            result_sent += ' [/E2] '\n",
    "        result_sent += sent[i]\n",
    "    if subj_range[1] == len(sent):\n",
    "        result_sent += ' [/E1]'\n",
    "    elif obj_range[1] == len(sent):\n",
    "        result_sent += ' [/E2]'\n",
    "\n",
    "\n",
    "    return result_sent.strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-budapest",
   "metadata": {
    "id": "dress-budapest"
   },
   "source": [
    "## Dataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-ghost",
   "metadata": {
    "id": "flush-ghost"
   },
   "outputs": [],
   "source": [
    "class KREDataset(Dataset):\n",
    "    \"\"\" Dataloader for Korean Relation Extraction Dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, data: pd.DataFrame, args):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.args = args\n",
    "        self.data = data\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained(args.bert_model)\n",
    "        # entity markers tokens\n",
    "        special_tokens_dict = {'additional_special_tokens': ['[E1]', '[/E1]', '[E2]', '[/E2]']}\n",
    "        num_added_toks = self.tokenizer.add_special_tokens(special_tokens_dict)   # num_added_toks: 4\n",
    "        # model.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "        self.max_token_len = args.max_token_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        data_row = self.data.iloc[idx]\n",
    "        \n",
    "        # input 문장\n",
    "        sentence = data_row.sentence\n",
    "        \n",
    "        # subj range, obj range\n",
    "        subj_range = [data_row['subj_start_pos'], data_row['subj_end_pos']]\n",
    "        obj_range = [data_row['obj_start_pos'], data_row['obj_end_pos']]\n",
    "        \n",
    "        # input 문장 변형 - entity markers 추가: entity_markers_added 함수 이용\n",
    "        converted_sent = entity_markers_added(sentence, subj_range, obj_range)\n",
    "        \n",
    "        labels = torch.FloatTensor(eval(data_row.label_onehot))\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            converted_sent,\n",
    "            add_special_tokens=True,\n",
    "            max_length = self.max_token_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        mask = encoding['attention_mask'].flatten()\n",
    "        \n",
    "        return dict(sentence=converted_sent,\n",
    "                   input_ids=input_ids,\n",
    "                   attention_mask=mask,\n",
    "                   labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-picnic",
   "metadata": {
    "id": "metallic-picnic"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-engine",
   "metadata": {
    "id": "dominican-engine"
   },
   "outputs": [],
   "source": [
    "class KREModel(pl.LightningModule):\n",
    "    \"\"\" Model for Multi-label classification for Korean Relation Extraction Dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, args, n_training_steps=None, n_warmup_steps=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.args = args\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained(args.bert_model, return_dict=True)\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained(args.bert_model)\n",
    "        # entity markers tokens\n",
    "        special_tokens_dict = {'additional_special_tokens': ['[E1]', '[/E1]', '[E2]', '[/E2]']}\n",
    "        num_added_toks = self.tokenizer.add_special_tokens(special_tokens_dict)   # num_added_toks: 4\n",
    "        \n",
    "        self.bert.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "        if self.args.mode == \"ALLCC\":\n",
    "            self.scale = 4\n",
    "        elif self.args.mode == \"ENTMARK\":\n",
    "            self.scale = 2\n",
    "            \n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size * self.scale, args.n_class)\n",
    "        \n",
    "        self.criterion = nn.BCELoss()\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        batch_size = input_ids.size()[0]\n",
    "        \n",
    "        bert_outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = bert_outputs.last_hidden_state\n",
    "        \n",
    "        # 모든 entity marker의 hidden states를 concat\n",
    "        if self.args.mode == \"ALLCC\":\n",
    "            h_start_pos_tensor = (input_ids == 20000).nonzero()\n",
    "            h_end_pos_tensor = (input_ids == 20001).nonzero()\n",
    "            t_start_pos_tensor = (input_ids == 20002).nonzero()\n",
    "            t_end_pos_tensor = (input_ids == 20003).nonzero()\n",
    "            \n",
    "            h_start_list = h_start_pos_tensor.tolist()\n",
    "            h_end_list = h_end_pos_tensor.tolist()\n",
    "            t_start_list = t_start_pos_tensor.tolist()\n",
    "            t_end_list = t_end_pos_tensor.tolist()\n",
    "            \n",
    "            special_token_idx = []\n",
    "            \n",
    "            # special_token_idx example: [[1, 9, 11, 19], [3, 5, 8, 12], ..]\n",
    "            for h_start, h_end, t_start, t_end in zip(h_start_list, h_end_list, t_start_list, t_end_list):\n",
    "                special_token_idx.append([h_start[1], h_end[1], t_start[1], t_end[1]])\n",
    "            \n",
    "            # concat_state shape: [batch size, hidden size * 4]\n",
    "            for i, idx_list in enumerate(special_token_idx):\n",
    "                if i == 0:\n",
    "                    concat_state = last_hidden_state[i, idx_list].flatten().unsqueeze(0)\n",
    "                else:\n",
    "                    concat_state = torch.cat([concat_state, last_hidden_state[i, idx_list].flatten().unsqueeze(0)], dim=0)\n",
    "            \n",
    "        elif self.args.mode == \"ENTMARK\":\n",
    "            h_start_pos_tensor = (input_ids == 20000).nonzero()\n",
    "#             h_end_pos_tensor = (input_ids == 20001).nonzero()\n",
    "            t_start_pos_tensor = (input_ids == 20002).nonzero()\n",
    "#             t_end_pos_tensor = (input_ids == 20003).nonzero()\n",
    "            \n",
    "            h_start_list = h_start_pos_tensor.tolist()\n",
    "#             h_end_list = h_end_pos_tensor.tolist()\n",
    "            t_start_list = t_start_pos_tensor.tolist()\n",
    "#             t_end_list = t_end_pos_tensor.tolist()\n",
    "            \n",
    "            special_token_idx = []\n",
    "        \n",
    "            # special_token_idx example: [[1, 11], [3, 8], ..]\n",
    "            for h_start, t_start in zip(h_start_list, t_start_list):\n",
    "                special_token_idx.append([h_start[1], t_start[1]])\n",
    "            \n",
    "            # concat_state shape: [batch size, hidden size * 2]\n",
    "            for i, idx_list in enumerate(special_token_idx):\n",
    "                if i == 0:\n",
    "                    concat_state = last_hidden_state[i, idx_list].flatten().unsqueeze(0)\n",
    "                else:\n",
    "                    concat_state = torch.cat([concat_state, last_hidden_state[i, idx_list].flatten().unsqueeze(0)], dim=0)\n",
    "        \n",
    "        output = self.classifier(concat_state)\n",
    "        output = torch.sigmoid(output)\n",
    "        \n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels)\n",
    "        return loss, output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        labels = []\n",
    "        predictions = []\n",
    "        for output in outputs:\n",
    "            for out_labels in output[\"labels\"].detach().cpu():\n",
    "                labels.append(out_labels)\n",
    "            for out_predictions in output[\"predictions\"].detach().cpu():\n",
    "                predictions.append(out_predictions)\n",
    "        labels = torch.stack(labels).int()\n",
    "        predictions = torch.stack(predictions)\n",
    "        \n",
    "        for i in range(self.args.n_class):\n",
    "            class_roc_auc = auroc(predictions[:, i], labels[:, i])\n",
    "            self.logger.experiment.add_scalar(f\"{str(i)}_roc_auc/Train\", class_roc_auc, self.current_epoch)\n",
    "            \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=2e-5)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                    num_warmup_steps=self.n_warmup_steps,\n",
    "                                                    num_training_steps=self.n_training_steps)\n",
    "     \n",
    "        return dict(optimizer=optimizer, lr_scheduler=dict(scheduler=scheduler, interval='step'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-empty",
   "metadata": {
    "id": "dress-empty"
   },
   "source": [
    "## Train Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-cinema",
   "metadata": {
    "id": "technical-cinema"
   },
   "outputs": [],
   "source": [
    "class KREDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.args = args\n",
    "        self.batch_size = args.batch_size\n",
    "        self.max_token_len = args.max_token_len\n",
    "        \n",
    "        self.train_df = pd.read_csv(args.train_data)\n",
    "        self.val_df = pd.read_csv(args.val_data)\n",
    "        self.test_df = pd.read_csv(args.test_data)\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = KREDataset(self.train_df, args)\n",
    "        self.val_dataset = KREDataset(self.val_df, args)\n",
    "        self.test_dataset = KREDataset(self.test_df, args)     \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.args.num_workers)\n",
    "                         \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.args.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-farming",
   "metadata": {
    "id": "minute-farming"
   },
   "outputs": [],
   "source": [
    "with open(args.relation_list, 'r') as f:\n",
    "    relation_list = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PUoL2dy4QjLy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PUoL2dy4QjLy",
    "outputId": "65b5d53c-4dc1-401a-8057-c34094ef1418"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P17', 'P131', 'P530', 'P150', 'P47']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-techno",
   "metadata": {
    "id": "introductory-techno"
   },
   "outputs": [],
   "source": [
    "def idx2class(idx_list):\n",
    "    label_out = []\n",
    "    \n",
    "    for idx in idx_list:\n",
    "        label = relation_list[idx]\n",
    "        label_out.append(label)\n",
    "    return label_out if label_out else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-grant",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afraid-grant",
    "outputId": "de63ad83-7d3f-4623-f664-7582197300f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P17', 'P131', 'P47']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2class([0,1,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-classroom",
   "metadata": {
    "id": "apart-classroom"
   },
   "outputs": [],
   "source": [
    "def evaluate(trained_model, test_dataset, log_filepath):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    trained_model = trained_model.to(device)\n",
    "    \n",
    "    predictions = []\n",
    "    labels = []\n",
    "    sentence_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for i, item in enumerate(tqdm(test_dataset)):\n",
    "        _, prediction = trained_model(\n",
    "            item[\"input_ids\"].unsqueeze(dim=0).to(device),\n",
    "            item[\"attention_mask\"].unsqueeze(dim=0).to(device)\n",
    "        )\n",
    "        \n",
    "        predictions.append(prediction.flatten())\n",
    "        labels.append(item[\"labels\"].int())\n",
    "        sentence_list.append(test_dataset[i]['sentence'])\n",
    "        label_list.append(idx2class(np.where(test_dataset[i]['labels'] == 1)[0]))\n",
    "    \n",
    "    predictions = torch.stack(predictions).detach().cpu()\n",
    "    labels = torch.stack(labels).detach().cpu()\n",
    "    \n",
    "    # [0.1, 0.2, .., 0.9]\n",
    "    threshold_list = np.arange(0, 1, 0.1)[1:]\n",
    "    \n",
    "    print(\"********** Accuracy per threshold **********\")\n",
    "    acc_per_threshold = dict()\n",
    "    for i in threshold_list:\n",
    "        acc = accuracy(predictions, labels, threshold=i)\n",
    "        acc_per_threshold[i] = acc\n",
    "        print(f\"Threshold: {i:.1f}, Accuracy: {acc:.6f}\")\n",
    "   \n",
    "    max_acc_threshold = sorted(acc_per_threshold.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "    print(f\"********** Max Threshold: {max_acc_threshold} **********\")\n",
    "    \n",
    "    print(\"********** Classification Report **********\")\n",
    "    y_pred = predictions.numpy()\n",
    "    y_true = labels.numpy()\n",
    "    upper, lower = 1, 0\n",
    "    y_pred = np.where(y_pred > max_acc_threshold, upper, lower)\n",
    "    \n",
    "    cls_report = classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        target_names=relation_list,\n",
    "        zero_division=0,\n",
    "        output_dict=True\n",
    "    )\n",
    "    print(classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        target_names=relation_list,\n",
    "        zero_division=0\n",
    "    ))\n",
    "    \n",
    "    # accuracy calculation\n",
    "    correct = 0\n",
    "    for i in range(len(predictions)):\n",
    "        for j in range(len(predictions[i])):\n",
    "            if predictions[i][j] < max_acc_threshold:\n",
    "                predictions[i][j] = 0\n",
    "            else:\n",
    "                predictions[i][j] = 1\n",
    "        if predictions[i].tolist() == labels[i].tolist():\n",
    "            correct += 1\n",
    "\n",
    "    acc = correct / len(predictions)\n",
    "    print(\"***********************************************************************\")\n",
    "    print(f\"단순 정확도(Accuracy): {acc*100:.4f}\")\n",
    "    print(f\"조항별 정확도를 고려한 전체 정확도(Weighted Avg의 F1 Accuracy): {cls_report['weighted avg']['f1-score']*100:.4f}\")\n",
    "    print(\"***********************************************************************\")\n",
    "    \n",
    "    ## 결과 csv 파일 저장\n",
    "    result_df = pd.DataFrame(columns=['sentence', 'relation_id', 'predicted_relation_id'])\n",
    "    \n",
    "    result_df['sentence'] = sentence_list\n",
    "    result_df['relation_id'] = label_list\n",
    "    \n",
    "    preds_list = []\n",
    "    for i in range(len(y_pred)):\n",
    "        class_pred = idx2class(np.where(y_pred[i] == 1)[0])\n",
    "        preds_list.append(class_pred)\n",
    "        \n",
    "    result_df['predicted_relation_id'] = preds_list\n",
    "    \n",
    "    # result_df.to_csv('../log/results.csv', index=False)\n",
    "    result_df.to_csv('/content/drive/MyDrive/Korean_RE/log/whole_data_results.csv')\n",
    "    \n",
    "    with open(log_filepath, 'w') as f:\n",
    "        # write args\n",
    "        f.write('********** Args **********\\n')\n",
    "        for k in list(vars(args).keys()):\n",
    "            f.write(f\"{k}: {vars(args)[k]}\\n\")\n",
    "\n",
    "        f.write('\\n********** Accuracy per threshold **********\\n')\n",
    "        for k, v in acc_per_threshold.items():\n",
    "            f.write(str(k)[:3] + '\\t' + str(v)[7:-1] + '\\n')\n",
    "        f.write('\\n')\n",
    "        f.write('********** Max Threshold: ' + str(max_acc_threshold) + ' **********\\n')\n",
    "       \n",
    "        f.write('\\n********** Classification Report **********\\n')\n",
    "        f.write('라벨이름\\tprecision\\trecall  \\tf1-score\\tsupport\\n')\n",
    "        for k, v in cls_report.items():\n",
    "            f.write(str(k) + '\\t')\n",
    "            f.write(f\"{v['precision']:.6f}\\t{v['recall']:.6f}\\t{v['f1-score']:.6f}\\t{v['support']}\" + '\\n')\n",
    "        f.write('\\n')\n",
    "        f.write(\"***********************************************************************\\n\")\n",
    "        f.write(f\"단순 정확도(Accuracy): {acc*100:.4f}\\n\")\n",
    "        f.write(f\"조항별 정확도를 고려한 전체 정확도(Weighted Avg의 F1 Accuracy): {cls_report['weighted avg']['f1-score']*100:.4f}\\n\")\n",
    "        f.write(\"***********************************************************************\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-limitation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "registered-limitation",
    "outputId": "3c6a2698-4d87-4d98-d94c-0fd6239c0c1b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# if not os.path.exists('../log'):\n",
    "#     os.mkdir('../log')\n",
    "    \n",
    "# if not os.path.exists('../ckpt'):\n",
    "#     os.mkdir('../ckpt')\n",
    "if not os.path.exists('/content/drive/MyDrive/Korean_RE/log'):\n",
    "    os.mkdir('/content/drive/MyDrive/Korean_RE/log')\n",
    "\n",
    "if not os.path.exists('/content/drive/MyDrive/Korean_RE/ckpt'):\n",
    "    os.mkdir('/content/drive/MyDrive/Korean_RE/ckpt')\n",
    "    \n",
    "pl.seed_everything(args.seed)\n",
    "\n",
    "data_module = KREDataModule(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c_SKuTJJtvo3",
   "metadata": {
    "id": "c_SKuTJJtvo3"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-lexington",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "8b9707139b2f4fdc920378556991338e",
      "a801b0f6f5944b3b8ca276111a288d69",
      "1c3274da30ab4610a907c239b36d6ef4",
      "f4a647e0699f449a827799c555858826",
      "8c29bc38eb2f4ea6a959bc98b50e6f05",
      "e91dbeb56b104d7ab24391aabc601dfe",
      "8c991b263f5049c3bfca43f8c2d8c804",
      "f14d8947b7874884b41030593d1f9646",
      "4e0a7e9bbb294008a7650d056301a812",
      "edc7a2bc96d5470aa9e2ae41dc6ebb4b",
      "f390b7c26d64447bb0898a727236cff5",
      "b0c8560785d749fcb5154a657124657b"
     ]
    },
    "id": "turned-lexington",
    "outputId": "18b7f51b-94f9-470e-a97a-546d5c2ee7df"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9707139b2f4fdc920378556991338e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/337 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a801b0f6f5944b3b8ca276111a288d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/408M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at snunlp/KR-Medium were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3274da30ab4610a907c239b36d6ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/143k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a647e0699f449a827799c555858826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type      | Params\n",
      "-----------------------------------------\n",
      "0 | bert       | BertModel | 101 M \n",
      "1 | classifier | Linear    | 298 K \n",
      "2 | criterion  | BCELoss   | 0     \n",
      "-----------------------------------------\n",
      "101 M     Trainable params\n",
      "0         Non-trainable params\n",
      "101 M     Total params\n",
      "406.810   Total estimated model params size (MB)\n",
      "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c29bc38eb2f4ea6a959bc98b50e6f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91dbeb56b104d7ab24391aabc601dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: `pos_label` automatically set 1.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c991b263f5049c3bfca43f8c2d8c804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 37243: val_loss reached 0.00591 (best 0.00591), saving model to \"/content/drive/MyDrive/Korean_RE/ckpt/whole_data/best-checkpoint.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f14d8947b7874884b41030593d1f9646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 74487: val_loss reached 0.00341 (best 0.00341), saving model to \"/content/drive/MyDrive/Korean_RE/ckpt/whole_data/best-checkpoint.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0a7e9bbb294008a7650d056301a812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 111731: val_loss reached 0.00280 (best 0.00280), saving model to \"/content/drive/MyDrive/Korean_RE/ckpt/whole_data/best-checkpoint.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc7a2bc96d5470aa9e2ae41dc6ebb4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 148975: val_loss reached 0.00263 (best 0.00263), saving model to \"/content/drive/MyDrive/Korean_RE/ckpt/whole_data/best-checkpoint.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f390b7c26d64447bb0898a727236cff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, step 186219: val_loss was not in top 1\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c8560785d749fcb5154a657124657b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 0.002592738252133131}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "if args.train:\n",
    "    steps_per_epoch = len(data_module.train_df) // args.batch_size\n",
    "    total_training_steps = steps_per_epoch * args.epochs\n",
    "    warmup_steps = total_training_steps // 5\n",
    "    \n",
    "    model = KREModel(args, n_training_steps=total_training_steps, n_warmup_steps=warmup_steps)\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=args.save_dir,\n",
    "        filename=\"best-checkpoint\",\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\"\n",
    "    )\n",
    "    \n",
    "    logger = TensorBoardLogger(\"lightning_logs\", name=\"KoreanRE\")\n",
    "    \n",
    "    early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=2)\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        logger=logger,\n",
    "        checkpoint_callback=checkpoint_callback,\n",
    "        callbacks=[early_stopping_callback],\n",
    "        max_epochs=args.epochs,\n",
    "        gpus=1,\n",
    "        progress_bar_refresh_rate=30\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, data_module)\n",
    "    \n",
    "    trainer.test()\n",
    "    \n",
    "    model.bert.save_pretrained(args.save_dir + '_bert')\n",
    "    \n",
    "# evaluate\n",
    "else:\n",
    "    test_dataset = KREDataset(data_module.test_df, args)\n",
    "    \n",
    "    trained_model = KREModel.load_from_checkpoint(os.path.join(args.save_dir, \"best-checkpoint.ckpt\"), args=args)\n",
    "    \n",
    "    trained_model.eval()\n",
    "    trained_model.freeze()\n",
    "    \n",
    "    evaluate(trained_model, test_dataset, args.log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AkGV3OeNtyPS",
   "metadata": {
    "id": "AkGV3OeNtyPS"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BnQjG1kzeYDq",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "BnQjG1kzeYDq",
    "outputId": "10cb1b93-b6a4-4b0b-a84d-bbcf2c457ba3",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at snunlp/KR-Medium were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 139666/139666 [40:12<00:00, 57.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Accuracy per threshold **********\n",
      "Threshold: 0.1, Accuracy: 0.998914\n",
      "Threshold: 0.2, Accuracy: 0.999111\n",
      "Threshold: 0.3, Accuracy: 0.999190\n",
      "Threshold: 0.4, Accuracy: 0.999227\n",
      "Threshold: 0.5, Accuracy: 0.999245\n",
      "Threshold: 0.6, Accuracy: 0.999252\n",
      "Threshold: 0.7, Accuracy: 0.999240\n",
      "Threshold: 0.8, Accuracy: 0.999207\n",
      "Threshold: 0.9, Accuracy: 0.999108\n",
      "********** Max Threshold: 0.6000000000000001 **********\n",
      "********** Classification Report **********\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         P17       0.98      0.98      0.98     23785\n",
      "        P131       0.98      0.98      0.98     15800\n",
      "        P530       1.00      0.99      1.00      7843\n",
      "        P150       1.00      0.99      0.99      7000\n",
      "         P47       0.99      0.98      0.98      6529\n",
      "        P106       0.99      0.99      0.99      6428\n",
      "         P27       0.94      0.95      0.95      6123\n",
      "        P461       1.00      0.99      0.99      4038\n",
      "        P279       0.95      0.93      0.94      3925\n",
      "        P495       0.94      0.95      0.95      3805\n",
      "        P641       1.00      0.99      1.00      3586\n",
      "        P156       0.99      0.96      0.98      3447\n",
      "        P155       0.98      0.96      0.97      3432\n",
      "        P527       0.95      0.94      0.95      2544\n",
      "        P361       0.92      0.93      0.93      2416\n",
      "       P1376       0.96      0.96      0.96      1895\n",
      "         P36       0.98      0.98      0.98      1768\n",
      "        P118       1.00      1.00      1.00      1295\n",
      "       P1889       0.92      0.92      0.92      1273\n",
      "         P31       0.94      0.87      0.90      1270\n",
      "        P175       0.95      0.91      0.93       990\n",
      "        P463       0.94      0.94      0.94       882\n",
      "         P54       0.99      0.98      0.99       880\n",
      "        P138       0.91      0.87      0.89       828\n",
      "         P81       0.99      0.99      0.99       802\n",
      "         P40       0.94      0.87      0.90       753\n",
      "        P159       0.95      0.83      0.88       739\n",
      "        P136       0.97      0.94      0.95       720\n",
      "        P171       0.99      0.98      0.99       662\n",
      "         P22       0.95      0.89      0.92       640\n",
      "         P26       0.92      0.91      0.92       623\n",
      "       P3373       0.90      0.89      0.90       621\n",
      "         P50       0.97      0.91      0.94       611\n",
      "         P30       1.00      0.99      0.99       594\n",
      "       P1532       0.85      0.67      0.75       593\n",
      "        P178       0.87      0.84      0.86       592\n",
      "        P413       1.00      1.00      1.00       590\n",
      "        P800       0.97      0.91      0.94       568\n",
      "       P1365       0.96      0.89      0.92       567\n",
      "        P276       0.86      0.77      0.81       550\n",
      "       P1366       0.96      0.88      0.92       536\n",
      "         P19       0.88      0.53      0.66       530\n",
      "        P449       0.94      0.91      0.92       523\n",
      "        P710       0.97      0.95      0.96       500\n",
      "       P2936       0.99      0.99      0.99       491\n",
      "       P1001       0.98      0.92      0.95       482\n",
      "        P140       0.98      0.96      0.97       450\n",
      "        P206       0.96      0.97      0.97       448\n",
      "       P1056       0.95      0.94      0.95       439\n",
      "         P20       0.79      0.78      0.78       422\n",
      "          P6       0.95      0.96      0.96       418\n",
      "        P123       0.92      0.89      0.90       417\n",
      "       P1830       0.94      0.80      0.87       411\n",
      "        P127       0.90      0.62      0.73       407\n",
      "       P1659       1.00      0.99      0.99       397\n",
      "        P112       0.93      0.91      0.92       391\n",
      "        P101       0.98      0.84      0.90       391\n",
      "       P3095       0.99      0.99      0.99       382\n",
      "        P749       0.83      0.83      0.83       349\n",
      "       P1696       1.00      1.00      1.00       345\n",
      "        P137       0.93      0.85      0.89       340\n",
      "       P2789       0.98      0.98      0.98       338\n",
      "        P706       0.94      0.84      0.89       336\n",
      "       P3842       1.00      0.97      0.98       327\n",
      "         P39       0.91      0.87      0.89       312\n",
      "        P425       0.99      0.99      0.99       303\n",
      "       P1336       0.98      0.97      0.97       301\n",
      "        P108       0.83      0.77      0.80       299\n",
      "        P172       0.97      0.96      0.97       298\n",
      "        P737       0.97      0.86      0.91       292\n",
      "        P176       0.90      0.86      0.88       288\n",
      "        P102       0.99      0.96      0.98       287\n",
      "         P35       0.96      0.91      0.93       286\n",
      "       P3730       1.00      0.99      0.99       263\n",
      "       P1687       1.00      0.99      0.99       251\n",
      "        P945       0.88      0.84      0.86       244\n",
      "        P264       0.99      0.96      0.98       243\n",
      "        P161       0.91      0.83      0.87       231\n",
      "         P69       0.86      0.77      0.81       231\n",
      "        P190       0.99      0.91      0.94       226\n",
      "        P355       0.89      0.87      0.88       226\n",
      "       P2341       0.95      0.89      0.92       224\n",
      "        P664       0.97      0.95      0.96       216\n",
      "        P407       0.97      0.98      0.97       211\n",
      "        P793       0.97      0.96      0.96       210\n",
      "        P840       0.94      0.63      0.75       202\n",
      "       P1441       0.96      0.84      0.89       202\n",
      "        P607       0.97      0.96      0.97       195\n",
      "        P197       1.00      1.00      1.00       194\n",
      "        P205       0.97      0.92      0.95       189\n",
      "        P162       0.65      0.53      0.58       173\n",
      "        P807       0.95      0.99      0.97       170\n",
      "       P1269       0.97      0.86      0.91       170\n",
      "        P170       0.91      0.65      0.76       168\n",
      "         P97       0.91      0.85      0.88       166\n",
      "        P750       0.81      0.68      0.74       164\n",
      "        P551       0.93      0.50      0.65       154\n",
      "\n",
      "   micro avg       0.97      0.96      0.96    139666\n",
      "   macro avg       0.95      0.90      0.92    139666\n",
      "weighted avg       0.97      0.96      0.96    139666\n",
      " samples avg       0.95      0.96      0.95    139666\n",
      "\n",
      "***********************************************************************\n",
      "단순 정확도(Accuracy): 95.2974\n",
      "조항별 정확도를 고려한 전체 정확도(Weighted Avg의 F1 Accuracy): 96.2525\n",
      "***********************************************************************\n"
     ]
    }
   ],
   "source": [
    "args.train = False\n",
    "\n",
    "# train\n",
    "if args.train:\n",
    "    steps_per_epoch = len(data_module.train_df) // args.batch_size\n",
    "    total_training_steps = steps_per_epoch * args.epochs\n",
    "    warmup_steps = total_training_steps // 5\n",
    "    \n",
    "    model = KREModel(args, n_training_steps=total_training_steps, n_warmup_steps=warmup_steps)\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=args.save_dir,\n",
    "        filename=\"best-checkpoint\",\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\"\n",
    "    )\n",
    "    \n",
    "    logger = TensorBoardLogger(\"lightning_logs\", name=\"KoreanRE\")\n",
    "    \n",
    "    early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=2)\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        logger=logger,\n",
    "        checkpoint_callback=checkpoint_callback,\n",
    "        callbacks=[early_stopping_callback],\n",
    "        max_epochs=args.epochs,\n",
    "        gpus=1,\n",
    "        progress_bar_refresh_rate=30\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, data_module)\n",
    "    \n",
    "    trainer.test()\n",
    "    \n",
    "    model.bert.save_pretrained(args.save_dir + '_bert')\n",
    "    \n",
    "# evaluate\n",
    "else:\n",
    "    test_dataset = KREDataset(data_module.test_df, args)\n",
    "    \n",
    "    trained_model = KREModel.load_from_checkpoint(os.path.join(args.save_dir, \"best-checkpoint.ckpt\"), args=args)\n",
    "    \n",
    "    trained_model.eval()\n",
    "    trained_model.freeze()\n",
    "    \n",
    "    evaluate(trained_model, test_dataset, args.log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RXUDzh1SyI6D",
   "metadata": {
    "id": "RXUDzh1SyI6D"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "whole_data_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
