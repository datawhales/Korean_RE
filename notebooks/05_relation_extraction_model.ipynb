{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "settled-walker",
   "metadata": {},
   "source": [
    "## Relation Extraction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "hundred-nightlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "### torch == 1.9.1\n",
    "### transformers == 4.10.3\n",
    "### pytorch_lightning == 1.2.8\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import easydict\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics.functional import accuracy, f1, auroc\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "impressed-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    'seed': 42\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "sound-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 12\n",
    "args.hidden_size = 768\n",
    "args.n_class = 97\n",
    "args.num_workers = 4\n",
    "args.epochs = 5\n",
    "args.train = True\n",
    "args.bert_model = 'snunlp/KR-Medium'\n",
    "args.max_token_len = 512\n",
    "args.train_data = '../data/toy_data_split/train.csv' \n",
    "args.val_data = '../data/toy_data_split/val.csv'\n",
    "args.test_data = '../data/toy_data_split/test.csv'\n",
    "args.relation_list = '../data/relation/relation_list.txt'\n",
    "args.save_dir = '../ckpt/'\n",
    "args.log_file = '../log/toy_result.txt'\n",
    "args.mode = \"ALLCC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acoustic-delaware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': 42,\n",
       " 'batch_size': 12,\n",
       " 'n_class': 97,\n",
       " 'num_workers': 4,\n",
       " 'epochs': 5,\n",
       " 'train': True,\n",
       " 'bert_model': 'snunlp/KR-Medium',\n",
       " 'max_token_len': 512,\n",
       " 'train_data': '../data/toy_data_split/train.csv',\n",
       " 'val_data': '../data/toy_data_split/val.csv',\n",
       " 'test_data': '../data/toy_data_split/test.csv',\n",
       " 'save_dir': '../ckpt/toy.pt',\n",
       " 'log_file': '../log/toy_result.txt'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "minute-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(args.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "digital-enhancement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>subj_name</th>\n",
       "      <th>subj_start_pos</th>\n",
       "      <th>subj_end_pos</th>\n",
       "      <th>subj_type</th>\n",
       "      <th>obj_name</th>\n",
       "      <th>obj_start_pos</th>\n",
       "      <th>obj_end_pos</th>\n",
       "      <th>obj_type</th>\n",
       "      <th>relation_id</th>\n",
       "      <th>relation_label</th>\n",
       "      <th>relation_type</th>\n",
       "      <th>relation_description</th>\n",
       "      <th>label_onehot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>모토로라 레이저 M는 모토로라 모빌리티에서 제조/판매하는 안드로이드 스마트폰이다.</td>\n",
       "      <td>모토로라 레이저 M</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>ARTIFACT</td>\n",
       "      <td>모토로라 모빌리티</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>P176</td>\n",
       "      <td>제조사</td>\n",
       "      <td>WikibaseItem</td>\n",
       "      <td>이 물건을 제조/제작한 주요 회사</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>웨인 페더먼은 미국의 배우, 텔레비전 배우, 각본가, 성우, 영화배우, 영화 프로듀...</td>\n",
       "      <td>웨인 페더먼</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>배우</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>OCCUPATION</td>\n",
       "      <td>P106</td>\n",
       "      <td>직업</td>\n",
       "      <td>WikibaseItem</td>\n",
       "      <td>항목 주제인 인물의 직업</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>공부하는 곳인 명륜당이 앞에, 사당인 대성전이 뒤에 있는 전학후묘의 형태로 향교의 ...</td>\n",
       "      <td>뒤</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>TERM</td>\n",
       "      <td>앞</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>TERM</td>\n",
       "      <td>P461</td>\n",
       "      <td>반대 개념</td>\n",
       "      <td>WikibaseItem</td>\n",
       "      <td>이 항목과 반대 관계의 항목</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>《2000 투데이》의 방송을 위해 영국의 BBC는 당시 총 5,000여명의 인력이 ...</td>\n",
       "      <td>텔레비전 센터</td>\n",
       "      <td>79</td>\n",
       "      <td>86</td>\n",
       "      <td>ARTIFACT</td>\n",
       "      <td>BBC</td>\n",
       "      <td>23</td>\n",
       "      <td>26</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>P127</td>\n",
       "      <td>소유자</td>\n",
       "      <td>WikibaseItem</td>\n",
       "      <td>항목 주제를 소유하고 있는 사람/단체</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>캐피틀 레코드가 미국에 비틀즈 음반을 판매하는 것을 거부하고 있던 것이 거슬렸던 브...</td>\n",
       "      <td>미국</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>COUNTRY</td>\n",
       "      <td>미국</td>\n",
       "      <td>67</td>\n",
       "      <td>69</td>\n",
       "      <td>COUNTRY</td>\n",
       "      <td>P17</td>\n",
       "      <td>다음 나라의 것임</td>\n",
       "      <td>WikibaseItem</td>\n",
       "      <td>항목 주제는 다음 나라(국가)의 것을 다루고 있음</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence   subj_name  \\\n",
       "0      모토로라 레이저 M는 모토로라 모빌리티에서 제조/판매하는 안드로이드 스마트폰이다.  모토로라 레이저 M   \n",
       "1  웨인 페더먼은 미국의 배우, 텔레비전 배우, 각본가, 성우, 영화배우, 영화 프로듀...      웨인 페더먼   \n",
       "2  공부하는 곳인 명륜당이 앞에, 사당인 대성전이 뒤에 있는 전학후묘의 형태로 향교의 ...           뒤   \n",
       "3  《2000 투데이》의 방송을 위해 영국의 BBC는 당시 총 5,000여명의 인력이 ...     텔레비전 센터   \n",
       "4  캐피틀 레코드가 미국에 비틀즈 음반을 판매하는 것을 거부하고 있던 것이 거슬렸던 브...          미국   \n",
       "\n",
       "   subj_start_pos  subj_end_pos subj_type   obj_name  obj_start_pos  \\\n",
       "0               0            10  ARTIFACT  모토로라 모빌리티             12   \n",
       "1               0             6    PERSON         배우             12   \n",
       "2              26            27      TERM          앞             13   \n",
       "3              79            86  ARTIFACT        BBC             23   \n",
       "4               9            11   COUNTRY         미국             67   \n",
       "\n",
       "   obj_end_pos      obj_type relation_id relation_label relation_type  \\\n",
       "0           21  ORGANIZATION        P176            제조사  WikibaseItem   \n",
       "1           14    OCCUPATION        P106             직업  WikibaseItem   \n",
       "2           14          TERM        P461          반대 개념  WikibaseItem   \n",
       "3           26  ORGANIZATION        P127            소유자  WikibaseItem   \n",
       "4           69       COUNTRY         P17      다음 나라의 것임  WikibaseItem   \n",
       "\n",
       "          relation_description  \\\n",
       "0           이 물건을 제조/제작한 주요 회사   \n",
       "1                항목 주제인 인물의 직업   \n",
       "2              이 항목과 반대 관계의 항목   \n",
       "3         항목 주제를 소유하고 있는 사람/단체   \n",
       "4  항목 주제는 다음 나라(국가)의 것을 다루고 있음   \n",
       "\n",
       "                                        label_onehot  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-ancient",
   "metadata": {},
   "source": [
    "## 각 문장을 BERT를 통과할 수 있는 형태로 변형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "informative-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_markers_added(sent: str, subj_range: list, obj_range: list) -> str:\n",
    "    \"\"\" 문장과 관계를 구하고자 하는 두 개체의 인덱스 범위가 주어졌을 때 entity marker token을 추가하여 반환하는 함수.\n",
    "    \n",
    "    Example:\n",
    "        sent = '모토로라 레이저 M는 모토로라 모빌리티에서 제조/판매하는 안드로이드 스마트폰이다.'\n",
    "        subj_range = [0, 10]   # sent[subj_range[0]: subj_range[1]] => '모토로라 레이저 M'\n",
    "        obj_range = [12, 21]   # sent[obj_range[0]: obj_range[1]] => '모토로라 모빌리티'\n",
    "        \n",
    "    Return:\n",
    "        '[E1] 모토로라 레이저 M [/E1] 는  [E2] 모토로라 모빌리티 [/E2] 에서 제조/판매하는 안드로이드 스마트폰이다.'\n",
    "    \"\"\"\n",
    "    result_sent = ''\n",
    "    \n",
    "    for i, char in enumerate(sent):\n",
    "        if i == subj_range[0]:\n",
    "            result_sent += ' [E1] '\n",
    "        elif i == subj_range[1]:\n",
    "            result_sent += ' [/E1] '\n",
    "        if i == obj_range[0]:\n",
    "            result_sent += ' [E2] '\n",
    "        elif i == obj_range[1]:\n",
    "            result_sent += ' [/E2] '\n",
    "        result_sent += sent[i]\n",
    "    if subj_range[1] == len(sent):\n",
    "        result_sent += ' [/E1]'\n",
    "    elif obj_range[1] == len(sent):\n",
    "        result_sent += ' [/E2]'\n",
    "    \n",
    "    return result_sent.strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "downtown-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(args.bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "promising-person",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens_dict = {'additional_special_tokens': ['[E1]', '[/E1]', '[E2]', '[/E2]']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "num_added_toks\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "together-neighborhood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='snunlp/KR-Medium', vocab_size=20000, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['[E1]', '[/E1]', '[E2]', '[/E2]']})"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "demanding-individual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20004"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "alike-techno",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_info = tokenizer.encode_plus(\n",
    "            converted_sent,\n",
    "            add_special_tokens=True,\n",
    "            max_length = args.max_token_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "voluntary-garlic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2, 20000,  2871,  5457,  5016,  5095, 14144,  5316,    49, 20001,\n",
       "         2367, 20002,  2871,  5457,  5016,  5095,  2871,  5821, 17420, 20003,\n",
       "         9235, 11532,    19,  9779,  8453,  3516, 13733, 12026, 12823,  8459,\n",
       "           18,     3,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = text_info['input_ids'].flatten()\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "twenty-election",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = text_info['attention_mask'].flatten()\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ignored-shannon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [E1] 모토로라 레이저 M [/E1] 는 [E2] 모토로라 모빌리티 [/E2] 에서 제조 / 판매하는 안드로이드 스마트폰이다. [SEP]'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer(converted_sent)['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-deputy",
   "metadata": {},
   "source": [
    "## Dataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "attached-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KREDataset(Dataset):\n",
    "    \"\"\" Dataloader for Korean Relation Extraction Dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, data: pd.DataFrame, args):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.args = args\n",
    "        self.data = data\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained(args.bert_model)\n",
    "        # entity markers tokens\n",
    "        special_tokens_dict = {'additional_special_tokens': ['[E1]', '[/E1]', '[E2]', '[/E2]']}\n",
    "        num_added_toks = self.tokenizer.add_special_tokens(special_tokens_dict)   # num_added_toks: 4\n",
    "        # model.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "        self.max_token_len = args.max_token_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        data_row = self.data.iloc[idx]\n",
    "        \n",
    "        # input 문장\n",
    "        sentence = data_row.sentence\n",
    "        \n",
    "        # subj range, obj range\n",
    "        subj_range = [data_row['subj_start_pos'], data_row['subj_end_pos']]\n",
    "        obj_range = [data_row['obj_start_pos'], data_row['obj_end_pos']]\n",
    "        \n",
    "        # input 문장 변형 - entity markers 추가: entity_markers_added 함수 이용\n",
    "        converted_sent = entity_markers_added(sentence, subj_range, obj_range)\n",
    "        \n",
    "        labels = torch.FloatTensor(eval(data_row.label_onehot))\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            converted_sent,\n",
    "            add_special_tokens=True,\n",
    "            max_length = self.max_token_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        mask = encoding['attention_mask'].flatten()\n",
    "        \n",
    "        return dict(sentence=converted_sent,\n",
    "                   input_ids=input_ids,\n",
    "                   attention_mask=mask,\n",
    "                   labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "amino-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "kredataset = KREDataset(train_df, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "whole-memorabilia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='snunlp/KR-Medium', vocab_size=20000, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['[E1]', '[/E1]', '[E2]', '[/E2]']})"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kredataset.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-wright",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "alien-mention",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-325-716dad114e2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mKREModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\" Model for Multi-label classification for Korean Relation Extraction Dataset.\n\u001b[1;32m      3\u001b[0m     \"\"\"\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_training_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pl' is not defined"
     ]
    }
   ],
   "source": [
    "class KREModel(pl.LightningModule):\n",
    "    \"\"\" Model for Multi-label classification for Korean Relation Extraction Dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, args, n_training_steps=None, n_warmup_steps=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.args = args\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained(args.bert_model, return_dict=True)\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained(args.bert_model)\n",
    "        # entity markers tokens\n",
    "        special_tokens_dict = {'additional_special_tokens': ['[E1]', '[/E1]', '[E2]', '[/E2]']}\n",
    "        num_added_toks = self.tokenizer.add_special_tokens(special_tokens_dict)   # num_added_toks: 4\n",
    "        \n",
    "        self.bert.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "        if self.args.mode == \"ALLCC\":\n",
    "            self.scale = 4\n",
    "        elif self.args.mode == \"ENTMARK\":\n",
    "            self.scale = 2\n",
    "            \n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size * self.scale, args.n_class)\n",
    "        \n",
    "        self.criterion = nn.BCELoss()\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        batch_size = input_ids.size()[0]\n",
    "        \n",
    "        bert_outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = bert_outputs.last_hidden_state\n",
    "        \n",
    "        # 모든 entity marker의 hidden states를 concat\n",
    "        if self.args.mode == \"ALLCC\":\n",
    "            h_start_pos_tensor = (input_ids == 20000).nonzero()\n",
    "            h_end_pos_tensor = (input_ids == 20001).nonzero()\n",
    "            t_start_pos_tensor = (input_ids == 20002).nonzero()\n",
    "            t_end_pos_tensor = (input_ids == 20003).nonzero()\n",
    "            \n",
    "            h_start_list = h_start_pos_tensor.tolist()\n",
    "            h_end_list = h_end_pos_tensor.tolist()\n",
    "            t_start_list = t_start_pos_tensor.tolist()\n",
    "            t_end_list = t_end_pos_tensor.tolist()\n",
    "            \n",
    "            special_token_idx = []\n",
    "            \n",
    "            # special_token_idx example: [[1, 9, 11, 19], [3, 5, 8, 12], ..]\n",
    "            for h_start, h_end, t_start, t_end in zip(h_start_list, h_end_list, t_start_list, t_end_list):\n",
    "                special_token_idx.append([h_start[1], h_end[1], t_start[1], t_end[1]])\n",
    "            \n",
    "            # concat_state shape: [batch size, hidden size * 4]\n",
    "            for i, idx_list in enumerate(special_token_idx):\n",
    "                if i == 0:\n",
    "                    concat_state = last_hidden_state[i, idx_list].flatten().unsqueeze(0)\n",
    "                else:\n",
    "                    concat_state = torch.cat([concat_state, last_hidden_state[i, idx_list].flatten().unsqueeze(0)], dim=0)\n",
    "            \n",
    "        elif self.args.mode == \"ENTMARK\":\n",
    "            h_start_pos_tensor = (input_ids == 20000).nonzero()\n",
    "#             h_end_pos_tensor = (input_ids == 20001).nonzero()\n",
    "            t_start_pos_tensor = (input_ids == 20002).nonzero()\n",
    "#             t_end_pos_tensor = (input_ids == 20003).nonzero()\n",
    "            \n",
    "            h_start_list = h_start_pos_tensor.tolist()\n",
    "#             h_end_list = h_end_pos_tensor.tolist()\n",
    "            t_start_list = t_start_pos_tensor.tolist()\n",
    "#             t_end_list = t_end_pos_tensor.tolist()\n",
    "            \n",
    "            special_token_idx = []\n",
    "        \n",
    "            # special_token_idx example: [[1, 11], [3, 8], ..]\n",
    "            for h_start, t_start in zip(h_start_list, t_start_list):\n",
    "                special_token_idx.append([h_start[1], t_start[1]])\n",
    "            \n",
    "            # concat_state shape: [batch size, hidden size * 2]\n",
    "            for i, idx_list in enumerate(special_token_idx):\n",
    "                if i == 0:\n",
    "                    concat_state = last_hidden_state[i, idx_list].flatten().unsqueeze(0)\n",
    "                else:\n",
    "                    concat_state = torch.cat([concat_state, last_hidden_state[i, idx_list].flatten().unsqueeze(0)], dim=0)\n",
    "        \n",
    "        output = self.classifier(concat_state)\n",
    "        output = torch.sigmoid(output)\n",
    "        \n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels)\n",
    "        return loss, output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        labels = []\n",
    "        predictions = []\n",
    "        for output in outputs:\n",
    "            for out_labels in output[\"labels\"].detach().cpu():\n",
    "                labels.append(out_labels)\n",
    "            for out_predictions in output[\"predictions\"].detach().cpu():\n",
    "                predictions.append(out_predictions)\n",
    "        labels = torch.stack(labels).int()\n",
    "        predictions = torch.stack(predictions)\n",
    "        \n",
    "        for i in range(self.args.n_class):\n",
    "            class_roc_auc = auroc(predictions[:, i], labels[:, i])\n",
    "            self.logger.experiment.add_scalar(f\"{str(i)}_roc_auc/Train\", class_roc_auc, self.current_epoch)\n",
    "            \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=2e-5)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                    num_warmup_steps=self.n_warmup_steps,\n",
    "                                                    num_training_steps=self.n_training_steps)\n",
    "     \n",
    "        return dict(optimizer=optimizer, lr_scheduler=dict(scheduler=scheduler, interval='step'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-expert",
   "metadata": {},
   "source": [
    "## Train Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "comic-lemon",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-330-aeb355f402d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mKREDataModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningDataModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pl' is not defined"
     ]
    }
   ],
   "source": [
    "class KREDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.args = args\n",
    "        self.batch_size = args.batch_size\n",
    "        self.max_token_len = args.max_token_len\n",
    "        \n",
    "        self.train_df = pd.read_csv(args.train_data)\n",
    "        self.val_df = pd.read_csv(args.val_data)\n",
    "        self.test_df = pd.read_csv(args.test_data)\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = KREDataset(self.train_df, args)\n",
    "        self.val_dataset = KREDataset(self.val_df, args)\n",
    "        self.test_dataset = KREDataset(self.test_df, args)     \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.args.num_workers)\n",
    "                         \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.args.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "tired-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.relation_list, 'r') as f:\n",
    "    relation_list = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-uganda",
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "republican-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2class(idx_list):\n",
    "    label_out = []\n",
    "    \n",
    "    for idx in idx_list:\n",
    "        label = relation_list[idx]\n",
    "        label_out.append(label)\n",
    "    return label_out if label_out else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "assured-moldova",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P17', 'P131', 'P47']"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2class([0,1,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(trained_model, test_dataset, log_filepath):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    trained_model = trained_model.to(device)\n",
    "    \n",
    "    predictions = []\n",
    "    labels = []\n",
    "    sentence_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for i, item in enumerate(tqdm(test_dataset)):\n",
    "        _, prediction = trained_model(\n",
    "            item[\"input_ids\"].unsqueeze(dim=0).to(device),\n",
    "            item[\"attention_mask\"].unsqueeze(dim=0).to(device)\n",
    "        )\n",
    "        \n",
    "        predictions.append(prediction.flatten())\n",
    "        labels.append(item[\"labels\"].int())\n",
    "        sentence_list.append(test_dataset[i]['sentence'])\n",
    "        label_list.append(idx2class(np.where(test_dataset[i]['labels'] == 1)[0]))\n",
    "    \n",
    "    predictions = torch.stack(predictions).detach().cpu()\n",
    "    labels = torch.stack(labels).detach().cpu()\n",
    "    \n",
    "    # [0.1, 0.2, .., 0.9]\n",
    "    threshold_list = np.arange(0, 1, 0.1)[1:]\n",
    "    \n",
    "    print(\"********** Accuracy per threshold **********\")\n",
    "    acc_per_threshold = dict()\n",
    "    for i in threshold_list:\n",
    "        acc = accuracy(predictions, labels, threshold=i)\n",
    "        acc_per_threshold[i] = acc\n",
    "        print(f\"Threshold: {i:.1f}, Accuracy: {acc:.6f}\")\n",
    "   \n",
    "    max_acc_threshold = sorted(acc_per_threshold.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "    print(f\"********** Max Threshold: {max_acc_threshold} **********\")\n",
    "    \n",
    "    print(\"********** Classification Report **********\")\n",
    "    y_pred = predictions.numpy()\n",
    "    y_true = labels.numpy()\n",
    "    upper, lower = 1, 0\n",
    "    y_pred = np.where(y_pred > max_acc_threshold, upper, lower)\n",
    "    \n",
    "    cls_report = classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        target_names=relation_list,\n",
    "        zero_division=0,\n",
    "        output_dict=True\n",
    "    )\n",
    "    print(classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        target_names=relation_list,\n",
    "        zero_division=0\n",
    "    ))\n",
    "    \n",
    "    # accuracy calculation\n",
    "    correct = 0\n",
    "    for i in range(len(predictions)):\n",
    "        for j in range(len(predictions[i])):\n",
    "            if predictions[i][j] < max_acc_threshold:\n",
    "                predictions[i][j] = 0\n",
    "            else:\n",
    "                predictions[i][j] = 1\n",
    "        if predictions[i].tolist() == labels[i].tolist():\n",
    "            correct += 1\n",
    "\n",
    "    acc = correct / len(predictions)\n",
    "    print(\"***********************************************************************\")\n",
    "    print(f\"단순 정확도(Accuracy): {acc*100:.4f}\")\n",
    "    print(f\"조항별 정확도를 고려한 전체 정확도(Weighted Avg의 F1 Accuracy): {cls_report['weighted avg']['f1-score']*100:.4f}\")\n",
    "    print(\"***********************************************************************\")\n",
    "    \n",
    "    ## 결과 csv 파일 저장\n",
    "    result_df = pd.DataFrame(columns=['sentence', 'relation_id', 'predicted_relation_id'])\n",
    "    \n",
    "    result_df['sentence'] = sentence_list\n",
    "    result_df['relation_id'] = label_list\n",
    "    \n",
    "    preds_list = []\n",
    "    for i in range(len(y_pred)):\n",
    "        class_pred = idx2class(np.where(y_pred[i] == 1)[0])\n",
    "        preds_list.append(class_pred)\n",
    "        \n",
    "    result_df['predicted_relation_id'] = preds_list\n",
    "    \n",
    "    result_df.to_csv('../log/results.csv', index=False)\n",
    "    \n",
    "    with open(log_filepath, 'w') as f:\n",
    "        # write args\n",
    "        f.write('********** Args **********\\n')\n",
    "        for k in list(vars(args).keys()):\n",
    "            f.write(f\"{k}: {vars(args)[k]}\\n\")\n",
    "\n",
    "        f.write('\\n********** Accuracy per threshold **********\\n')\n",
    "        for k, v in acc_per_threshold.items():\n",
    "            f.write(str(k)[:3] + '\\t' + str(v)[7:-1] + '\\n')\n",
    "        f.write('\\n')\n",
    "        f.write('********** Max Threshold: ' + str(max_acc_threshold) + ' **********\\n')\n",
    "       \n",
    "        f.write('\\n********** Classification Report **********\\n')\n",
    "        f.write('라벨이름\\tprecision\\trecall  \\tf1-score\\tsupport\\n')\n",
    "        for k, v in cls_report.items():\n",
    "            f.write(str(k) + '\\t')\n",
    "            f.write(f\"{v['precision']:.6f}\\t{v['recall']:.6f}\\t{v['f1-score']:.6f}\\t{v['support']}\" + '\\n')\n",
    "        f.write('\\n')\n",
    "        f.write(\"***********************************************************************\\n\")\n",
    "        f.write(f\"단순 정확도(Accuracy): {acc*100:.4f}\\n\")\n",
    "        f.write(f\"조항별 정확도를 고려한 전체 정확도(Weighted Avg의 F1 Accuracy): {cls_report['weighted avg']['f1-score']*100:.4f}\\n\")\n",
    "        f.write(\"***********************************************************************\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "differential-clearance",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-337-e481f51e1d53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdata_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKREDataModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pl' is not defined"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('../log'):\n",
    "    os.mkdir('../log')\n",
    "    \n",
    "if not os.path.exists('../ckpt'):\n",
    "    os.mkdir('../ckpt')\n",
    "    \n",
    "pl.seed_everything(args.seed)\n",
    "\n",
    "data_module = KREDataModule(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "cultural-disco",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_module' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-338-46cca3420b2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtotal_training_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwarmup_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_training_steps\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_module' is not defined"
     ]
    }
   ],
   "source": [
    "# train\n",
    "if args.train:\n",
    "    steps_per_epoch = len(data_module.train_df) // args.batch_size\n",
    "    total_training_steps = steps_per_epoch * args.epochs\n",
    "    warmup_steps = total_training_steps // 5\n",
    "    \n",
    "    model = KREModel(args, n_training_steps=total_training_steps, n_warmup_steps=warmup_steps)\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=args.save_dir,\n",
    "        filename=\"best-checkpoint\",\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\"\n",
    "    )\n",
    "    \n",
    "    logger = TensorBoardLogger(\"lightning_logs\", name=\"KoreanRE\")\n",
    "    \n",
    "    early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=2)\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        logger=logger,\n",
    "        checkpoint_callback=checkpoint_callback,\n",
    "        callbacks=[early_stopping_callback],\n",
    "        max_epochs=args.epochs,\n",
    "        gpus=1,\n",
    "        progress_bar_refresh_rate=30\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, data_module)\n",
    "    \n",
    "    trainer.test()\n",
    "    \n",
    "    model.bert.save_pretrained(args.save_dir + '_bert')\n",
    "    \n",
    "# evaluate\n",
    "else:\n",
    "    test_dataset = KREDataset(data_module.test_df, args)\n",
    "    \n",
    "    trained_model = KREModel.load_from_checkpoint(os.path.join(args.save_dir, \"best-checkpoint.ckpt\"), args=args)\n",
    "    \n",
    "    trained_model.eval()\n",
    "    trained_model.freeze()\n",
    "    \n",
    "    evaluate(trained_model, test_dataset, args.log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.train = False\n",
    "\n",
    "# train\n",
    "if args.train:\n",
    "    steps_per_epoch = len(data_module.train_df) // args.batch_size\n",
    "    total_training_steps = steps_per_epoch * args.epochs\n",
    "    warmup_steps = total_training_steps // 5\n",
    "    \n",
    "    model = KREModel(args, n_training_steps=total_training_steps, n_warmup_steps=warmup_steps)\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=args.save_dir,\n",
    "        filename=\"best-checkpoint\",\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\"\n",
    "    )\n",
    "    \n",
    "    logger = TensorBoardLogger(\"lightning_logs\", name=\"KoreanRE\")\n",
    "    \n",
    "    early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=2)\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        logger=logger,\n",
    "        checkpoint_callback=checkpoint_callback,\n",
    "        callbacks=[early_stopping_callback],\n",
    "        max_epochs=args.epochs,\n",
    "        gpus=1,\n",
    "        progress_bar_refresh_rate=30\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, data_module)\n",
    "    \n",
    "    trainer.test()\n",
    "    \n",
    "    model.bert.save_pretrained(args.save_dir + '_bert')\n",
    "    \n",
    "# evaluate\n",
    "else:\n",
    "    test_dataset = KREDataset(data_module.test_df, args)\n",
    "    \n",
    "    trained_model = KREModel.load_from_checkpoint(os.path.join(args.save_dir, \"best-checkpoint.ckpt\"), args=args)\n",
    "    \n",
    "    trained_model.eval()\n",
    "    trained_model.freeze()\n",
    "    \n",
    "    evaluate(trained_model, test_dataset, args.log_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
